{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datathon Analysis Template\n",
    "\n",
    "**Project:** Inter-Uni Datathon 2025  \n",
    "**Author:** [Your Name]  \n",
    "**Date:** [Date]  \n",
    "**Objective:** [Describe the analysis objective]\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Configuration](#setup)\n",
    "2. [Data Loading & Exploration](#data-loading)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Exploratory Data Analysis](#eda)\n",
    "5. [Feature Engineering](#feature-engineering)\n",
    "6. [Model Development](#modeling)\n",
    "7. [Results & Evaluation](#results)\n",
    "8. [Conclusions & Next Steps](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
    "\n",
    "# Custom utilities\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils import load_and_inspect, quick_eda, create_baseline_plots\n",
    "from preprocessing import DataPreprocessor, create_features\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration {#data-loading}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# Replace 'your_dataset.csv' with actual dataset path\n",
    "data_path = '../data/raw/your_dataset.csv'\n",
    "\n",
    "# Uncomment and modify based on your dataset\n",
    "# df = load_and_inspect(data_path)\n",
    "\n",
    "print(\"üìä Data loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape if 'df' in locals() else 'Load your dataset first'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data exploration\n",
    "if 'df' in locals():\n",
    "    # Display first few rows\n",
    "    display(df.head())\n",
    "    \n",
    "    # Data types and missing values\n",
    "    print(\"\\nüìã Data Info:\")\n",
    "    df.info()\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"\\n‚ùó Missing Values:\")\n",
    "        print(missing_data[missing_data > 0].sort_values(ascending=False))\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing {#preprocessing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "target_column = 'your_target_column'  # Replace with actual target\n",
    "\n",
    "# Initialize preprocessor\n",
    "if 'df' in locals():\n",
    "    preprocessor = DataPreprocessor()\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    df_processed = preprocessor.fit_transform(df, target_column)\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing complete!\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Processed shape: {df_processed.shape}\")\n",
    "    \n",
    "    # Display processed data sample\n",
    "    display(df_processed.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Load dataset first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis {#eda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "if 'df_processed' in locals():\n",
    "    print(\"üìä Statistical Summary:\")\n",
    "    display(df_processed.describe())\n",
    "    \n",
    "    # Quick EDA using utility function\n",
    "    quick_eda(df_processed, target_column)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Process data first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "if 'df_processed' in locals():\n",
    "    create_baseline_plots(df_processed, target_column)\n",
    "    \n",
    "    # Additional custom plots based on your data\n",
    "    # Add domain-specific visualizations here\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering {#feature-engineering}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "if 'df_processed' in locals():\n",
    "    df_features = create_features(df_processed)\n",
    "    \n",
    "    print(f\"‚úÖ Feature engineering complete!\")\n",
    "    print(f\"Original features: {df_processed.shape[1]}\")\n",
    "    print(f\"With new features: {df_features.shape[1]}\")\n",
    "    \n",
    "    # Display new feature names\n",
    "    new_features = set(df_features.columns) - set(df_processed.columns)\n",
    "    if new_features:\n",
    "        print(f\"\\nüÜï New features created: {list(new_features)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Process data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Development {#modeling}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "if 'df_features' in locals() and target_column in df_features.columns:\n",
    "    X = df_features.drop(columns=[target_column])\n",
    "    y = df_features[target_column]\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, \n",
    "        stratify=y if len(y.unique()) < 10 else None\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data split complete!\")\n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Prepare features first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and evaluation\n",
    "if 'X_train' in locals():\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # Determine problem type\n",
    "    is_classification = len(y.unique()) < 10 and y.dtype == 'object' or y.dtype == 'int64'\n",
    "    \n",
    "    if is_classification:\n",
    "        print(\"üéØ Classification Problem Detected\")\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "        }\n",
    "        metric_name = 'Accuracy'\n",
    "    else:\n",
    "        print(\"üìà Regression Problem Detected\")\n",
    "        models = {\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'Linear Regression': LinearRegression()\n",
    "        }\n",
    "        metric_name = 'R¬≤ Score'\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüîÑ Training {name}...\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "        \n",
    "        # Fit and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if is_classification:\n",
    "            score = accuracy_score(y_test, predictions)\n",
    "        else:\n",
    "            from sklearn.metrics import r2_score\n",
    "            score = r2_score(y_test, predictions)\n",
    "        \n",
    "        results[name] = {\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'test_score': score,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        print(f\"CV {metric_name}: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        print(f\"Test {metric_name}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model training complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Prepare training data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results & Evaluation {#results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "if 'results' in locals():\n",
    "    print(\"üìä Model Comparison:\")\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'CV Score': [results[model]['cv_mean'] for model in results.keys()],\n",
    "        'CV Std': [results[model]['cv_std'] for model in results.keys()],\n",
    "        'Test Score': [results[model]['test_score'] for model in results.keys()]\n",
    "    })\n",
    "    \n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Best model\n",
    "    best_model_name = comparison_df.loc[comparison_df['Test Score'].idxmax(), 'Model']\n",
    "    best_model = results[best_model_name]['model']\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "    print(f\"Test Score: {results[best_model_name]['test_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if available)\n",
    "if 'best_model' in locals() and hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüîç Top 10 Feature Importances:\")\n",
    "    display(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation for best model\n",
    "if 'best_model' in locals() and 'is_classification' in locals():\n",
    "    predictions = best_model.predict(X_test)\n",
    "    \n",
    "    if is_classification:\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "        \n",
    "        print(\"\\nüìã Classification Report:\")\n",
    "        print(classification_report(y_test, predictions))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "    else:\n",
    "        from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        print(f\"\\nüìä Regression Metrics:\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        # Residual plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        residuals = y_test - predictions\n",
    "        plt.scatter(predictions, residuals, alpha=0.6)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Predicted Values')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title('Residual Plot')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions & Next Steps {#conclusions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "- [ ] Summary of main insights from the data\n",
    "- [ ] Model performance summary\n",
    "- [ ] Most important features identified\n",
    "- [ ] Business implications\n",
    "\n",
    "### Next Steps\n",
    "- [ ] Hyperparameter tuning for best model\n",
    "- [ ] Try additional algorithms (XGBoost, Neural Networks)\n",
    "- [ ] Feature engineering improvements\n",
    "- [ ] Cross-validation strategy refinement\n",
    "- [ ] Deploy model for production use\n",
    "\n",
    "### Presentation Points\n",
    "- [ ] Problem statement and approach\n",
    "- [ ] Data insights and preprocessing steps\n",
    "- [ ] Model comparison and selection\n",
    "- [ ] Business recommendations\n",
    "- [ ] Future work and improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "if 'best_model' in locals():\n",
    "    # Create models directory\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    \n",
    "    # Save best model\n",
    "    model_filename = f'../models/best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved: {model_filename}\")\n",
    "    \n",
    "    # Save results summary\n",
    "    results_summary = {\n",
    "        'best_model': best_model_name,\n",
    "        'test_score': results[best_model_name]['test_score'],\n",
    "        'cv_score': results[best_model_name]['cv_mean'],\n",
    "        'feature_count': X.shape[1],\n",
    "        'training_samples': X_train.shape[0],\n",
    "        'test_samples': X_test.shape[0]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìÑ Final Results Summary:\")\n",
    "    for key, value in results_summary.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nüéâ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datathon 2025",
   "language": "python",
   "name": "datathon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
